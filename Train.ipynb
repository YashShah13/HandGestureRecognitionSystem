{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Lib.data_loader import DataLoader\n",
    "from Lib.utils import mkdirs\n",
    "from Lib.resnet_model import Resnet3DBuilder\n",
    "from Lib.HistoryGraph import Historygraph\n",
    "import Lib.image as img\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import ceil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers import SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_size =(64,96)\n",
    "nb_frames= 16\n",
    "skip = 1\n",
    "nb_classes = 27\n",
    "batch_size = 64\n",
    "input_shape =(nb_frames,) + target_size + (3,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "workers = 8\n",
    "use_multiprocessing = False\n",
    "max_queue_size = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "# Path_videos\n",
    "#Path_labels\n",
    "#Path_train\n",
    "#Path_validation\n",
    "#Path_test\n",
    "# Path_videos\n",
    "#Path_labels\n",
    "#Path_train\n",
    "#Path_validation\n",
    "#Path_test\n",
    "class DataLoader():\n",
    "    def __init__(self, path_vid, path_labels, path_train=None, path_val=None, path_test=None):\n",
    "        self.path_vid=path_vid\n",
    "        self.path_labels=path_labels\n",
    "        self.path_train=path_train\n",
    "        self.path_val=path_val\n",
    "        self.path_test=path_test\n",
    "        \n",
    "        \n",
    "        self.get_labels(path_labels)\n",
    "            \n",
    "        if self.path_train:\n",
    "            self.train_df= self.load_video_labels(self.path_train)\n",
    "        if self.path_val:\n",
    "            self.val_df= self.load_video_labels(self.path_val)\n",
    "        if self.path_test:\n",
    "            self.test_df= self.load_video_labels(self.path_test,mode=\"input\")\n",
    "            \n",
    "            \n",
    "    def get_labels(self, path_labels):\n",
    "        \n",
    "        \n",
    "        self.labels_df = pd.read_csv(path_labels, names=['Label'])\n",
    "        #Extract data from dataFrames\n",
    "        self.labels= [str(label[0]) for label in self.labels_df.values]\n",
    "        #print(self.labels)\n",
    "        self.n_labels= len(self.labels)\n",
    "        #print(self.n_labels)\n",
    "        #create Dictionaries to convert label to int and backward\n",
    "        self.label_to_int= dict(zip(self.labels, range(self.n_labels))) \n",
    "       # print(self.label_to_int)\n",
    "        self.init_to_labels= dict(enumerate(self.labels))\n",
    "        #print(self.init_to_labels)\n",
    "        #dictionary={0:swipingLeft}\n",
    "        \n",
    "    \n",
    "        \n",
    "    def load_video_labels(self, path_subset,mode=\"label\"):\n",
    "        if mode == \"input\":\n",
    "            names= [\"video_id\"]\n",
    "        elif mode == \"label\":\n",
    "            names =[\"video_id\", \"label\"]\n",
    "        \n",
    "        \n",
    "        \n",
    "        df=pd.read_csv(path_subset, sep=';', names=names)\n",
    "        \n",
    "        if mode == \"label\":\n",
    "            df = df[df.label.isin(self.labels)]\n",
    "        print(df)\n",
    "            \n",
    "            \n",
    "        return df\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_root = r\"B:\\Dataset\\labels\"\n",
    "csv_labels = r\"B:\\Dataset\\labels\\jester-v1-labels.csv\"\n",
    "csv_train = r\"B:\\Dataset\\labels\\jester-v1-train.csv\"\n",
    "csv_val = r\"B:\\Dataset\\labels\\jester-v1-validation.csv\"\n",
    "csv_test =r\"B:\\Dataset\\labels\\jester-v1-test.csv\"\n",
    "data_vid=r\"B:\\Dataset\\labels\\videos\"\n",
    "model_name= \"resent_3d_model\"\n",
    "data_model=r\"B:\\Dataset\\labels\\model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_model=os.path.join(data_root, data_model, model_name)\n",
    "path_vid=os.path.join(data_root,data_vid)\n",
    "path_labels=os.path.join(data_root,csv_labels)\n",
    "path_train=os.path.join(data_root,csv_train)\n",
    "path_val=os.path.join(data_root, csv_val)\n",
    "path_test=os.path.join(data_root, csv_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        video_id                      label\n",
      "0          34870           Drumming Fingers\n",
      "1          56557  Sliding Two Fingers Right\n",
      "2         129112   Sliding Two Fingers Down\n",
      "3          63861     Pulling Two Fingers In\n",
      "4         131717     Sliding Two Fingers Up\n",
      "...          ...                        ...\n",
      "118557     75507               Swiping Down\n",
      "118558     48433   Sliding Two Fingers Left\n",
      "118559    146421  Sliding Two Fingers Right\n",
      "118560     49514                   Thumb Up\n",
      "118561      4502     Sliding Two Fingers Up\n",
      "\n",
      "[118562 rows x 2 columns]\n",
      "       video_id                      label\n",
      "0          9223                   Thumb Up\n",
      "1        107090   Pushing Two Fingers Away\n",
      "2         42920               Swiping Left\n",
      "3        106485                 Thumb Down\n",
      "4        142201      Rolling Hand Backward\n",
      "...         ...                        ...\n",
      "14782     97044           Drumming Fingers\n",
      "14783    136208  Sliding Two Fingers Right\n",
      "14784     12180      Rolling Hand Backward\n",
      "14785    119381                 Thumb Down\n",
      "14786     64033                 Swiping Up\n",
      "\n",
      "[14787 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "data = DataLoader(path_vid, path_labels, path_train, path_val)\n",
    "mkdirs(path_model, 0o755)\n",
    "mkdirs(os.path.join(path_model, \"graphs\"), 0o755)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 118562 video folders belonging to 27 classes.\n",
      "Found 14787 video folders belonging to 27 classes.\n"
     ]
    }
   ],
   "source": [
    "gen=img.ImageDataGenerator()\n",
    "gen_train= gen.flow_video_from_dataframe(data.train_df,path_vid,path_classes= path_labels,x_col='video_id',y_col=\"labels\", target_size=target_size,batch_size=batch_size,nb_frames=nb_frames, skip=skip, has_ext=True)\n",
    "gen_val= gen.flow_video_from_dataframe(data.val_df,path_vid,path_classes= path_labels,x_col='video_id',y_col=\"labels\", target_size=target_size,batch_size=batch_size,nb_frames=nb_frames, skip=skip, has_ext=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet_model = Resnet3DBuilder.build_resnet_101(input_shape, nb_classes, drop_rate = 0.5)\n",
    "optimizer = SGD(lr=0.01, momentum=0.9, decay=0.0001, nesterov=False)\n",
    "resnet_model.compile(optimizer = optimizer, loss=\"categorical_crossentrophy\", metrics=[\"accuracy\"])\n",
    "model_file= os.path.join(path_model, \"resnetmodel.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpointer = ModelCheckpoint(model_file, monitor='val_acc', verbose=1, save_best_only=True, mode='max')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_graph = Historygraph(model_path_name = os.path.join(path_model, \"graphs\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_sample_train = data.train_df[\"video_id\"].size\n",
    "nb_sample_val = data.val_df[\"video_id\"].size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] The system cannot find the path specified: 'B:\\\\Dataset\\\\labels\\\\videos\\\\39876'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-b115949f8671>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m resnet_model.fit(\n\u001b[0m\u001b[0;32m      2\u001b[0m                  \u001b[0mgen_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m                  \u001b[0msteps_per_epoch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mceil\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnb_sample_train\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m                  \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m                  \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mgen_val\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\yash shah\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1048\u001b[0m          \u001b[0mtraining_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mRespectCompiledTrainableState\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1049\u001b[0m       \u001b[1;31m# Creates a `tf.data.Dataset` and handles batch and epoch iteration.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1050\u001b[1;33m       data_handler = data_adapter.DataHandler(\n\u001b[0m\u001b[0;32m   1051\u001b[0m           \u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1052\u001b[0m           \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\yash shah\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model, steps_per_execution)\u001b[0m\n\u001b[0;32m   1098\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1099\u001b[0m     \u001b[0madapter_cls\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mselect_data_adapter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1100\u001b[1;33m     self._adapter = adapter_cls(\n\u001b[0m\u001b[0;32m   1101\u001b[0m         \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1102\u001b[0m         \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\yash shah\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, x, y, sample_weights, shuffle, workers, use_multiprocessing, max_queue_size, model, **kwargs)\u001b[0m\n\u001b[0;32m    900\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_keras_sequence\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    901\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_enqueuer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 902\u001b[1;33m     super(KerasSequenceAdapter, self).__init__(\n\u001b[0m\u001b[0;32m    903\u001b[0m         \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    904\u001b[0m         \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;31m# Shuffle is handed in the _make_callable override.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\yash shah\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, x, y, sample_weights, workers, use_multiprocessing, max_queue_size, model, **kwargs)\u001b[0m\n\u001b[0;32m    777\u001b[0m     \u001b[1;31m# Since we have to know the dtype of the python generator when we build the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    778\u001b[0m     \u001b[1;31m# dataset, we have to look at a batch to infer the structure.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 779\u001b[1;33m     \u001b[0mpeek\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_peek_and_restore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    780\u001b[0m     \u001b[0mpeek\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_standardize_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpeek\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    781\u001b[0m     \u001b[0mpeek\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_tensorlike\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpeek\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\yash shah\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\data_adapter.py\u001b[0m in \u001b[0;36m_peek_and_restore\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m    911\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    912\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_peek_and_restore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 913\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    914\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    915\u001b[0m   def _handle_multiprocessing(self, x, workers, use_multiprocessing,\n",
      "\u001b[1;32mB:\\AI_Projects\\HandGestureProject\\Lib\\image.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m   1534\u001b[0m         index_array = self.index_array[self.batch_size * idx:\n\u001b[0;32m   1535\u001b[0m                                        self.batch_size * (idx + 1)]\n\u001b[1;32m-> 1536\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_batches_of_transformed_samples\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex_array\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1537\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1538\u001b[0m     def common_init(self, image_data_generator,\n",
      "\u001b[1;32mB:\\AI_Projects\\HandGestureProject\\Lib\\image.py\u001b[0m in \u001b[0;36m_get_batches_of_transformed_samples\u001b[1;34m(self, index_array)\u001b[0m\n\u001b[0;32m   2201\u001b[0m             \u001b[0mfname\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfilenames\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2202\u001b[0m             \u001b[0mfolder\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2203\u001b[1;33m             \u001b[0mfiles\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfolder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2204\u001b[0m             \u001b[1;31m#TODO: At the moment the number of image sample per video is given as parameter and\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2205\u001b[0m             \u001b[1;31m#TODO: if there are not enough frames, the remaining space will stay empty ( full of 0 )\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] The system cannot find the path specified: 'B:\\\\Dataset\\\\labels\\\\videos\\\\39876'"
     ]
    }
   ],
   "source": [
    "resnet_model.fit(\n",
    "                 gen_train,\n",
    "                 steps_per_epoch = ceil(nb_sample_train/batch_size),\n",
    "                 epochs=100,\n",
    "                 validation_data=gen_val,\n",
    "                 validation_steps=30,\n",
    "                 shuffle=True,\n",
    "                 verbose=1,\n",
    "                 workers= workers,\n",
    "                 max_queue_size= max_queue_size,\n",
    "                 use_multiprocessing= use_multiprocessing,\n",
    "                 callbacks=[model_checkpointer,history_graph])\n",
    "                                \n",
    "                         "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
